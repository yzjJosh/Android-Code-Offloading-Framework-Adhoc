In previous 2 weeks we have completed all tasks defined in tasks4.txt. Our current design has already taken care of fault tolerance, thus we mainly focused on load balancing in these 2 weeks. We have improved our load balancing strategy by considering more metrics including the CPU usage and network throughput of each node. Finally we have tested our load balancing strategy by simulating 100 nodes and found that this strategy can evenly distribute requests among nodes.

Our design can tolerant faults including crashing and connection losses. What we do in case of failure is running the method in local device for another time. Because the invoking object and method parameters are modified in the server side, changes are not committed to local device until the remote method invocation completes. Thus if we rerun the method in case of failure, the results will still be correct as long as these objects are not touched by other local threads.

When it comes to load balancing, our previous round-robin design is problematic. What we did is running a scheduler inside each client node. When a client wants to send a request, the scheduler will select a server node in round-robin manner. However if a server node is busy running other processes, it is not wise to send a lot of requests to that node. This scenario may happen in real world because our nodes are personal devices like smartphones or tablets, which may be heavily used by the user. In this case the round-robin strategy may fail because client does not know user is using that server for other tasks.

In order to solve this problem, we have designed another scheduler which schedule tasks in a cleverer manner. We firstly collect server metrics, including the total CPU percentage, requests received per second, and size of data (in bytes) received and sent per second. These metrics are collected to clients in a periodical manner. Then our clients can consider these metrics when it wants to schedule a task to a server.

Our first strategy is to choose the least busy server. We firstly try to choose the server with the lowest CPU percentage. If some servers have the same CPU percentage, we use requests per second to break tie. If servers still have same requests per second, we finally use data size to break tie. This strategy should be better than round-robin strategy because it measures the server load. If a server is busy running other tasks or other processes, the CPU percentage, requests per second, and data size received and sent should be high. Thus our scheduler can avoid these busy servers and choose others.

Surprisingly, the above strategy is worse than the round-robin strategy in our test. That is due to the metric collecting mechanism. Though all these metrics are changed continuously, we can only sample these information in a discrete manner. Due to performance reason, we cannot collect these metrics too frequently. Thus our metrics are updated every 5 seconds. This cause the least busy server keep receiving requests for 5 seconds before its metric being updated. We can relieve this problem by shortening the period of collecting metrics, but it does not solve the problem.

To solve this problem, we have designed second strategy. We random pick 3 servers from all the available servers, and choose the least busy server from picked ones. This strategy can solve the previous problem when server number is big. Because it is very unlikely that a server is selected too many times. But if the number of server is small, e.g. 3, this strategy degrades to the first one.

Finally, we merge our random-selection strategy and the round-robin strategy together to solve the problem. When the server number is small (less than 10), we use round-robin strategy. When the server number is big, we use the random-selection strategy. In this way we can solve the load balancing problem in spite of the server number.

It is hard to test our scheduler in real devices or virtual machines because we need a large number of nodes. Thus we do unit test on our scheduler class to see how it works. To test our strategy, we add 100 virtual nodes into our scheduler. In order to simulate the load, we add the CPU percentage of a node by 0.1 when it is selected by the scheduler. Initially the load of nodes are all 0. We make the scheduler collect CPU load periodically. We schedule the servers 10,000 times, and find that all the servers are selected similar frequently. This shows that our strategy works well theoretically.